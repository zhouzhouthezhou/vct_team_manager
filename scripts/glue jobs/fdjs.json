{
	"jobConfig": {
		"name": "fdjs",
		"description": "",
		"role": "arn:aws:iam::054037119284:role/glue_role",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.4X",
		"numberOfWorkers": 10,
		"maxCapacity": 40,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "fdjs.py",
		"scriptLocation": "s3://aws-glue-assets-054037119284-us-west-2/scripts/",
		"language": "python-3",
		"spark": false,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-10-19T05:21:35.312Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-054037119284-us-west-2/temporary/",
		"etlAutoScaling": true,
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-054037119284-us-west-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nimport time\nimport os\nimport logging\nimport sys\nimport os.path\nimport requests\nimport boto3\nimport json\n\nfrom collections import deque\nfrom io import BytesIO\nfrom enum import Enum, auto\nfrom os import listdir\nfrom decimal import Decimal\n\nimport pandas as pd\nimport numpy as np\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import input_file_name, udf, from_json, col, explode, row_number\nfrom pyspark.sql.types import StringType, ArrayType, StructType, StructField, DoubleType\nfrom pyspark.sql.window import Window\n\nlogging.basicConfig(\n    format='{asctime} [{levelname}] {message}',\n    style=\"{\",\n    datefmt=\"%H:%M\",\n    level=logging.CRITICAL,\n    force=True\n)\n\nbucket = \"actualvctdata\"\ns3 = boto3.client('s3')\n\nLEAGUE = 'vct-challengers'\nYEAR = 2024\n\ndef add_item_to_dynamodb(table_name, item):\n    table = dynamodb.Table(table_name)\n    response = table.put_item(Item=item)\n    return response\n\ndef read_json_from_s3(bucket_name, file):\n    response = s3.get_object(Bucket=bucket_name, Key=file)\n    content = response['Body'].read().decode('utf-8')\n\n    return json.loads(content)\n\ndef list_s3_files(bucket_name, prefix):\n    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n\n    files = []\n    if 'Contents' in response:\n        for obj in response['Contents']:\n            files.append(obj['Key'])\n\n    return files\n\nmaps = {\n    \"Infinity\": 'ABYSS',\n    \"Ascent\": 'ASCENT',\n    \"Duality\": 'BIND',\n    \"Foxtrot\": 'BREEZE',\n    \"Canyon\": 'FRACTURE',\n    \"Triad\": 'HAVEN',\n    \"Port\": 'ICEBOX',\n    \"Jam\": 'LOTUS',\n    \"Pitt\": 'PEARL',\n    \"Bonsai\": 'SPLIT',\n    \"Juliett\": 'SUNSET',\n}\n\n'''\n    'AFFINITY_ABYSS',\n    'AFFINITY_ASCENT',\n    'AFFINITY_BIND',\n    'AFFINITY_BREEZE',\n    'AFFINITY_FRACTURE',\n    'AFFINITY_HAVEN',\n    'AFFINITY_ICEBOX',\n    'AFFINITY_LOTUS',\n    'AFFINITY_PEARL',\n    'AFFINITY_SPLIT',\n    'AFFINITY_SUNSET',\n'''\n\nvec_fields = [\n    'ROUND_NUMBER',\n    'OUTCOME',\n    'SIDE',\n    'KILLS',\n    'DEATHS',\n    'ASSISTS',\n    'COMBAT_SCORE',\n    'KILLS_STINGER',\n    'KILLS_BUCKY',\n    'KILLS_JUDGE',\n    'KILLS_SPECTRE',\n    'KILLS_BULLDOG',\n    'KILLS_GUARDIAN',\n    'KILLS_PHANTOM',\n    'KILLS_VANDAL',\n    'KILLS_MARSHAL',\n    'KILLS_OUTLAW',\n    'KILLS_OPERATOR',\n    'KILLS_ARES',\n    'KILLS_ODIN',\n    'KILLS_CLASSIC',\n    'KILLS_SHORTY',\n    'KILLS_FRENZY',\n    'KILLS_GHOST',\n    'KILLS_SHERIFF',\n    'KILLS_MELEE',\n    'TIME_ALIVE',\n    'DEAD',\n    'DAMAGE_TAKEN',\n    'DAMAGE_DONE',\n    'SPIKE_CARRY_PERCENT',\n    'SPIKE_PLANT',\n    'ASTRA_PICK_RATE',\n    'BREACH_PICK_RATE',\n    'BRIMSTONE_PICK_RATE',\n    'CHAMBER_PICK_RATE',\n    'CYPHER_PICK_RATE',\n    'DEADLOCK_PICK_RATE',\n    'FADE_PICK_RATE',\n    'GEKKO_PICK_RATE',\n    'HARBOR_PICK_RATE',\n    'JETT_PICK_RATE',\n    'KAYO_PICK_RATE',\n    'KILLJOY_PICK_RATE',\n    'NEON_PICK_RATE',\n    'OMEN_PICK_RATE',\n    'PHOENIX_PICK_RATE',\n    'RAZE_PICK_RATE',\n    'REYNA_PICK_RATE',\n    'SAGE_PICK_RATE',\n    'SKYE_PICK_RATE',\n    'SOVA_PICK_RATE',\n    'VIPER_PICK_RATE',\n    'YORU_PICK_RATE',\n    'ISO_PICK_RATE',\n    'CLOVE_PICK_RATE',\n    'VYSE_PICK_RATE',\n    'DUELIST_PICK_RATE',\n    'INITIATOR_PICK_RATE',\n    'SENTINEL_PICK_RATE',\n    'CONTROLLER_PICK_RATE',\n    # TODO: map score\n    # TODO: win type\n]\n\nclass PlayerRound:\n    def __init__(self, game_id, player_id, map):\n        # TODO: add abilities, player killed data, more damage data\n        self.metadata = {\n            'game_id': game_id,\n            'map': map,\n        }\n        self.player_id = player_id\n        self.vec = dict()\n        for v in vec_fields:\n            self.vec[v] = 0\n\n    def update_vec(self, idx, val):\n        self.vec[idx] = val\n\n    def add_vec(self, idx, i):\n        self.vec[idx] += i\n\n    def get_vec(self, idx):\n        return self.vec[idx]\n\n    def flatten(self):\n        for v in vec_fields:\n            self.vec[v] = float(str(self.vec[v]))\n        self.vec['id'] = self.player_id\n        self.vec['metadata'] = self.metadata\n        return self.vec\n\nclass Game:\n    def _process_event(self, event):\n        if 'snapshot' in event:\n            return\n\n        # agent_name, agent_class, side, round number\n        if 'roundStarted' in event:\n            e = event['roundStarted']\n            logging.debug(f'Round started {e}')\n\n            self._processing_round = True\n            self._curr_round_start_time = float(event['metadata']['eventTime']['omittingPauses'][:-1])\n\n            attacking_team = str(e['spikeMode']['attackingTeam']['value'])\n            # agent_name and agent_class\n            for i, p in enumerate(self.player_loc.values()):\n                pi = str(i+1)\n                self.players[pi]['player_round'] = PlayerRound(self.game_id, p, self.map)\n\n                agent = self.players[pi]['agent_name'] + '_PICK_RATE'\n                agent_class = self.players[pi]['agent_role'] + '_PICK_RATE'\n\n                self.players[pi]['player_round'].update_vec(agent, 1)\n                self.players[pi]['player_round'].update_vec(agent_class, 1)\n\n                # Set current round number\n                self.players[pi]['player_round'].update_vec('ROUND_NUMBER', e['roundNumber'])\n\n                # Set side\n                if int(pi) in self.teams[attacking_team]['players']:\n                    self.players[pi]['player_round'].update_vec('SIDE', 1)\n                else:\n                    self.players[pi]['player_round'].update_vec('SIDE', -1)\n\n            return\n\n        # Skip processing if not inside of a round\n        if not self._processing_round:\n            return\n\n        cur_time = float(event['metadata']['eventTime']['omittingPauses'][:-1])\n\n        # damage receive, damage dealt\n        if 'damageEvent' in event:\n            e = event['damageEvent']\n            logging.debug(f'Damage Event {e}')\n\n            # Set damage dealt\n            if 'causerId' in e:\n                causer = str(e['causerId']['value'])\n                self.players[causer]['player_round'].add_vec('DAMAGE_DONE', e['damageAmount'])\n\n            # Set damage received\n            victim = str(e['victimId']['value'])\n            self.players[victim]['player_round'].add_vec('DAMAGE_TAKEN', e['damageAmount'])\n\n            return\n\n        # death flag, weapon kill, time alive, kills, deaths, asissts,\n        if 'playerDied' in event:\n            e = event['playerDied']\n            time_stamp = float(event['metadata']['eventTime']['omittingPauses'][:-1])\n            logging.debug(f'Player Died {e}')\n\n            # Set death flag and death counter\n            dead_player = str(e['deceasedId']['value'])\n            self.players[dead_player]['player_round'].update_vec('DEAD', 1)\n            self.players[dead_player]['player_round'].add_vec('DEATHS', 1)\n\n            # Set time alive\n            time_alive = time_stamp - self._curr_round_start_time\n            self.players[dead_player]['player_round'].update_vec('TIME_ALIVE', time_alive)\n\n            # Update weapon kill tracker and kill counter\n            killer = str(e['killerId']['value'])\n            self.players[killer]['player_round'].add_vec('KILLS', 1)\n            if 'weapon' in e:\n                weapon_guid = e['weapon']['fallback']['guid']\n                if weapon_guid == \"\":\n                    self.players[killer]['player_round'].add_vec('KILLS_MELEE', 1)\n                else:\n                    g = requests.get(f'https://valorant-api.com/v1/weapons/{weapon_guid}')\n                    wkey = 'KILLS_' + g.json()['data']['displayName'].upper()\n                    self.players[killer]['player_round'].add_vec(wkey, 1)\n\n            # Update assist counter\n            if 'assistants' in e:\n                for a in e['assistants']:\n                    assister = str(a['assistantId']['value'])\n                    self.players[assister]['player_round'].add_vec('ASSISTS', 1)\n            return\n\n        # spike plant, spike carry time, spike defuse\n        if 'spikeStatus' in event:\n            e = event['spikeStatus']\n            logging.debug(f'Spike Status {e}')\n\n            # Set spike plant flag and update spike carry time and spike defuse flag\n            if e['status'] == \"IN_HANDS\" and 'carrier' in e:\n                if not 'carrier' in e:\n                    logging.warning(\"SPIKE IN_HANDS event with no carrier found\")\n                else:\n                    self._curr_spike_carrier = str(e['carrier']['value'])\n                self._curr_spike_pickup_stamp = float(event['metadata']['eventTime']['omittingPauses'][:-1])\n            elif e['status'] == \"PLANTED\" and self._curr_spike_carrier is not None:\n                self.players[self._curr_spike_carrier]['player_round'].update_vec('SPIKE_PLANT', 1)\n                self.players[self._curr_spike_carrier]['player_round'].add_vec('SPIKE_CARRY_PERCENT', cur_time - self._curr_spike_pickup_stamp)\n            elif e['status'] == \"ON_GROUND\" and self._curr_spike_carrier is not None:\n                self.players[self._curr_spike_carrier]['player_round'].add_vec('SPIKE_CARRY_PERCENT', cur_time - self._curr_spike_pickup_stamp)\n\n            return\n\n        # combat score, outcome, time alive, noramlize spike carry time\n        if 'roundDecided' in event:\n            e = event['roundDecided']\n            logging.debug(f'Round Decided {e}')\n\n            round_length = cur_time - self._curr_round_start_time\n            winning_team = str(e['result']['winningTeam']['value'])\n            for p in self.players:\n                # Set outcome\n                if int(p) in self.teams[winning_team]['players']:\n                    self.players[p]['player_round'].update_vec('OUTCOME', 1)\n                else:\n                    self.players[p]['player_round'].update_vec('OUTCOME', -1)\n\n                # Set time alive\n                if self.players[p]['player_round'].get_vec('DEAD') == 0:\n                    self.players[p]['player_round'].add_vec('TIME_ALIVE', round_length)\n\n                # Normalize spike time\n                spike_time = self.players[p]['player_round'].get_vec('SPIKE_CARRY_PERCENT')\n                self.players[p]['player_round'].update_vec('SPIKE_CARRY_PERCENT', spike_time / round_length)\n\n\n            round_end_stamp = float(event['metadata']['eventTime']['omittingPauses'][:-1])\n            while 'snapshot' not in event:\n                event = self.event_feed.popleft()\n\n            e = event['snapshot']\n\n            # Set combat score\n            for p in e['players']:\n                player = str(p['playerId']['value'])\n                self.players[player]['player_round'].update_vec('COMBAT_SCORE', p['scores']['combatScore']['roundScore'])\n\n            self._processing_round = False\n            self._curr_round_start_time = None\n            self._curr_spike_carrier = None\n            self._curr_spike_pickup_stamp = None\n            self._curr_round_start_time = None\n\n            for p in self.players.values():\n                self.round_vecs.append(p['player_round'].flatten())\n\n            return\n\n\n    def __init__(self, data, use_file=False):\n        self.players = dict()\n        self.teams = dict()\n        self._curr_round_start_time = None\n        self._curr_spike_carrier = None\n        self._curr_spike_pickup_stamp = None\n        self._curr_round_start_time = None\n        self._processing_round = False\n        self.round_vecs = list()\n\n        if use_file:\n            #j = read_json_from_s3(bucket, data)\n            pass\n        else:\n            j = json.loads(data)\n        \n        self.event_feed = deque(j)\n\n        first_event = self.event_feed.popleft()\n\n        self.game_id = first_event['platformGameId']\n\n        self.player_loc = mapping_df.loc[mapping_df['platformGameId'] == self.game_id, 'participantMapping'].values[0]\n\n        second_event = self.event_feed.popleft()\n\n        for i, p in enumerate(self.player_loc.values()):\n            self.players[str(i+1)] = {\n                'player_round': None,\n                'agent_name': \"\",\n                'agent_role': \"\",\n            }\n\n        self.map = maps[second_event['configuration']['selectedMap']['fallback']['displayName']]\n\n        self.player_agents = dict()\n        for i, p in enumerate(second_event['configuration']['players']):\n            agent_guid = p['selectedAgent']['fallback']['guid']\n            agent_data = requests.get(f'https://valorant-api.com/v1/agents/{agent_guid}')\n            self.players[str(p['playerId']['value'])]['agent_name'] = agent_data.json()['data']['displayName'].upper()\n            self.players[str(p['playerId']['value'])]['agent_role'] = agent_data.json()['data']['role']['displayName'].upper()\n\n        teamid = str(second_event['configuration']['teams'][0]['teamId']['value'])\n        self.teams[teamid] = dict()\n        self.teams[teamid]['players'] = [p['value'] for p in second_event['configuration']['teams'][0]['playersInTeam']]\n        self.teams[teamid]['name'] = team_df.iloc[second_event['configuration']['teams'][0]['teamId']['value']]['slug']\n\n        teamid = str(second_event['configuration']['teams'][1]['teamId']['value'])\n        self.teams[teamid] = dict()\n        self.teams[teamid]['players'] = [p['value'] for p in second_event['configuration']['teams'][1]['playersInTeam']]\n        self.teams[teamid]['name'] = team_df.iloc[second_event['configuration']['teams'][1]['teamId']['value']]['slug']\n\n#         # ingest events\n        # logging.info(f\"Ingesting events for {self.name}\")\n        while len(self.event_feed) != 0:\n            current_event = self.event_feed.popleft()\n            self._process_event(current_event)\n        # logging.info(f\"Done ingesting events for {self.name}\")\n    \n    def get_rounds(self):\n        return self.round_vecs\n\ndef consume_game(r):\n    g = Game(r)\n    return g.get_rounds()\n\ndef process_data(glueContext, s3_path):\n    spark = glueContext.spark_session\n    \n    # Read all file paths and assign row numbers\n    df = spark.read.text(s3_path).select(input_file_name().alias(\"file_path\")).distinct()\n    w = Window.orderBy(\"file_path\")\n    df_with_row_num = df.withColumn(\"row_num\", row_number().over(w))\n    df_with_row_num.show(n=1)\n    \n    # Get total number of files\n    total_files = df_with_row_num.count()\n    \n    # Define schema for better memory management\n    json_schema = ArrayType(StructType([StructField(v, DoubleType(), True) for v in vec_fields] + \n                                       [StructField('metadata', StringType(), True), \n                                        StructField('id', StringType(), True)]))\n    \n    process_udf = udf(consume_game, json_schema)\n    \n    # Process data in batches\n    batch_size = 85  # Process 50 files at a time\n    for i in range(0, total_files, batch_size):\n        # Get batch of file paths\n        batch_df = df_with_row_num.filter((col(\"row_num\") > i) & (col(\"row_num\") <= i + batch_size))\n        batch_paths = batch_df.select(\"file_path\").collect()\n        batch_paths = [row.file_path for row in batch_paths]\n        \n        # Read and process batch\n        batch_data_df = spark.read.text(batch_paths)\n        processed_df = batch_data_df.withColumn(\"processed_data\", process_udf(batch_data_df.value))\n        exploded_df = processed_df.select(explode(\"processed_data\").alias(\"exploded_data\"))\n        \n        # Expand the struct into individual columns\n        expanded_df = exploded_df.select(*[col(f'exploded_data.{f}').alias(f) for f in db_fields])\n        \n        # Write batch results\n        expanded_df.write.mode(\"append\").parquet(f\"s3://{bucket}/{LEAGUE}/parquet/{YEAR}.parquet\")\n\n\nmapping_df = pd.DataFrame(read_json_from_s3(bucket, f'{LEAGUE}/esports-data/mapping_data.json'))\nteam_df = pd.DataFrame(read_json_from_s3(bucket, f'{LEAGUE}/esports-data/teams.json'))\n\n## @params: [JOB_NAME]\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\n\ns3_path = f\"s3a://{bucket}/{LEAGUE}/games/{YEAR}/*.json\"\n#vct-international/games/2022/val004b09b1-4dc9-4185-baff-9b1c66b3ef99.json\n\ndb_fields = vec_fields + ['id', 'metadata']\n\nif __name__ == \"__main__\":\n    args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n    \n    sc = SparkContext()\n    glueContext = GlueContext(sc)\n    job = Job(glueContext)\n    job.init(args['JOB_NAME'], args)\n    \n    start = time.time()\n    \n    process_data(glueContext, s3_path)\n        \n    print(f\"Processing completed in {time.time() - start} seconds\")\n    job.commit()\n\n# start = time.time()\n# df = spark.read.text(s3_path).repartition('value')\n\n# db_fields = vec_fields + ['id', 'metadata']\n\n# json_schema = ArrayType(StructType([StructField(v, DoubleType(), False) for v in vec_fields] + [StructField('metadata', StringType(), False), StructField('id', StringType(), False)]))\n\n# # process_udf = udf(consume_game, json_schema)\n\n# # processed_df = df.withColumn(\"processed_data\", process_udf(df.value))\n# # # processed_df.show(n=1)\n\n# # exploded_df = processed_df.withColumn(\"exploded_data\", explode(\"processed_data\"))\n# # # exploded_df.show(n=1)\n\n# # # Expand the struct into individual columns\n# # expanded_df = exploded_df.select(*[col(f'exploded_data.{f}').alias(f) for f in db_fields])\n\n# # expanded_df.write.parquet(f\"s3a://{bucket}/{LEAGUE}/parquet/{YEAR}.parquet\",mode=\"overwrite\")\n\n# batch_size = 50  # Adjust based on your memory constraints\n# for i in range(0, df.count(), batch_size):\n#     batch_df = df.limit(batch_size).offset(i)\n        \n#     processed_df = batch_df.withColumn(\"processed_data\", process_udf(batch_df.value))\n#     exploded_df = processed_df.select(explode(\"processed_data\").alias(\"exploded_data\"))\n        \n#     # Expand the struct into individual columns\n#     expanded_df = exploded_df.select(*[col(f'exploded_data.{f}').alias(f) for f in db_fields])\n        \n#     # Write batch results\n#     expanded_df.write.mode(\"append\").parquet(f\"s3://{bucket}/{LEAGUE}/parquet/{YEAR}.parquet\")\n# print(time.time() - start)\n\n# job.commit()"
}